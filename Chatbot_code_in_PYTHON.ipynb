{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Nn89PqrWTjed"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"W2hGju9aGrjs"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"9Zj9h92wUDPN"},"source":["**Importing the required libraries**"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1527,"status":"ok","timestamp":1644387186411,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"4BmyocFbb0MJ"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'nltk'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\BHAVYA SREE\\OneDrive\\Desktop\\Bhavya\\Chatbot-in-Python\\Chatbot_code_in_PYTHON.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/BHAVYA%20SREE/OneDrive/Desktop/Bhavya/Chatbot-in-Python/Chatbot_code_in_PYTHON.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/BHAVYA%20SREE/OneDrive/Desktop/Bhavya/Chatbot-in-Python/Chatbot_code_in_PYTHON.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/BHAVYA%20SREE/OneDrive/Desktop/Bhavya/Chatbot-in-Python/Chatbot_code_in_PYTHON.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/BHAVYA%20SREE/OneDrive/Desktop/Bhavya/Chatbot-in-Python/Chatbot_code_in_PYTHON.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"]}],"source":["import numpy as np\n","import nltk\n","import string\n","import random"]},{"cell_type":"markdown","metadata":{"id":"85QE5FDSUKqU"},"source":["**Importing and reading the corpus**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1557,"status":"ok","timestamp":1644387242592,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"jouIkYEkb9Pk","outputId":"647db158-fda3-4ef1-e99d-d8abdb7cd13b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}],"source":["f=open('chatbot.txt','r',errors = 'ignore')\n","raw_doc=f.read()\n","raw_doc=raw_doc.lower() #Converts text to lowercase\n","nltk.download('punkt') #Using the Punkt tokenizer\n","nltk.download('wordnet') #Using the WordNet dictionary\n","sent_tokens = nltk.sent_tokenize(raw_doc) #Converts doc to list of sentences \n","word_tokens = nltk.word_tokenize(raw_doc) #Converts doc to list of words"]},{"cell_type":"markdown","metadata":{"id":"pmXgGkVeUSUb"},"source":["**Example of sentance tokens**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":406,"status":"ok","timestamp":1644387246131,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"Swu4WRVncPR8","outputId":"8b1b89e8-9283-44e3-c333-826939392adb"},"outputs":[{"data":{"text/plain":["['data science\\nfrom wikipedia, the free encyclopedia\\njump to navigationjump to search\\nnot to be confused with information science.',\n"," 'the existence of comet neowise (here depicted as a series of red dots) was discovered by analyzing astronomical survey data acquired by a space telescope, the wide-field infrared survey explorer.']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["sent_tokens[:2]"]},{"cell_type":"markdown","metadata":{"id":"Gtkzd0KhUWJT"},"source":["**Example of word tokens**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1644387249974,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"hcwrvmWicaLc","outputId":"31a9615d-fb0a-4a08-fdeb-57c5802f06e6"},"outputs":[{"data":{"text/plain":["['data', 'science']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["word_tokens[:2]"]},{"cell_type":"markdown","metadata":{"id":"bvvYcZZ9UbVD"},"source":["**Text preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":703,"status":"ok","timestamp":1644387252621,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"YbZllVqBcc78"},"outputs":[],"source":["lemmer = nltk.stem.WordNetLemmatizer()\n","#WordNet is a semantically-oriented dictionary of English included in NLTK.\n","def LemTokens(tokens):\n","    return [lemmer.lemmatize(token) for token in tokens]\n","remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n","def LemNormalize(text):\n","    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"]},{"cell_type":"markdown","metadata":{"id":"tLX8WBE4UgOr"},"source":["**Defining the greeting function**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1644387254415,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"dLOqphibchJM"},"outputs":[],"source":["GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n","GREET_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n","def greet(sentence):\n"," \n","    for word in sentence.split():\n","        if word.lower() in GREET_INPUTS:\n","            return random.choice(GREET_RESPONSES)"]},{"cell_type":"markdown","metadata":{"id":"qJhFmyRCUm4j"},"source":["**Response generation**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1644387256272,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"eo7Kv52HcjG0"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1644387258184,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"JEHZesw3cnNM"},"outputs":[],"source":["def response(user_response):\n","  robo1_response=''\n","  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n","  tfidf = TfidfVec.fit_transform(sent_tokens)\n","  vals = cosine_similarity(tfidf[-1], tfidf)\n","  idx=vals.argsort()[0][-2]\n","  flat = vals.flatten()\n","  flat.sort()\n","  req_tfidf = flat[-2]\n","  if(req_tfidf==0):\n","    robo1_response=robo1_response+\"I am sorry! I don't understand you\"\n","    return robo1_response\n","  else:\n","    robo1_response = robo1_response+sent_tokens[idx]\n","    return robo1_response"]},{"cell_type":"markdown","metadata":{"id":"1Q-iY_o1Utas"},"source":["**Defining conversation start/end protocols**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94212,"status":"ok","timestamp":1644387354576,"user":{"displayName":"Sunny Sree3","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gia05Oy0Xl4X6Mza6pH4ZdBAgShZDLEC_LyhOM4qA=s64","userId":"10510659426327499284"},"user_tz":-330},"id":"wxzENVDgdNGd","outputId":"73950c19-a1dd-4180-a404-14a4b2956dd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["BOT: My name is Stark. Let's have a conversation! Also, if you want to exit any time, just type Bye!\n","hi\n","BOT: hi\n","what is data science?\n","BOT: "]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"name":"stdout","output_type":"stream","text":["however, data science is different from computer science and information science.\n","data scientist\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"name":"stdout","output_type":"stream","text":["BOT: [4][5]\n","\n","a data scientist is someone who creates programming code, and combines it with statistical knowledge to create insights from data.\n","contents\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"name":"stdout","output_type":"stream","text":["BOT: [6]\n","\n","\n","contents\n","1\tfoundations\n","1.1\trelationship to statistics\n","2\tetymology\n","2.1\tearly usage\n","2.2\tmodern usage\n","3\ttechnologies and techniques\n","4\tsee also\n","5\treferences\n","foundations\n","data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains.\n","relationship to statistics\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  % sorted(inconsistent)\n"]},{"name":"stdout","output_type":"stream","text":["BOT: [12]\n","\n","relationship to statistics\n","many statisticians, including nate silver, have argued that data science is not a new field, but rather another name for statistics.\n","bye\n","BOT: Goodbye! Take care <3 \n"]}],"source":["flag=True\n","print(\"BOT: My name is Stark. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n","while(flag==True):\n","    user_response = input()\n","    user_response=user_response.lower()\n","    if(user_response!='bye'):\n","        if(user_response=='thanks' or user_response=='thank you' ):\n","            flag=False\n","            print(\"BOT: You are welcome..\")\n","        else:\n","            if(greet(user_response)!=None):\n","                print(\"BOT: \"+greet(user_response))\n","            else:\n","                sent_tokens.append(user_response)\n","                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n","                final_words=list(set(word_tokens))\n","                print(\"BOT: \",end=\"\")\n","                print(response(user_response))\n","                sent_tokens.remove(user_response)\n","    else:\n","        flag=False\n","        print(\"BOT: Goodbye! Take care <3 \")"]},{"cell_type":"markdown","metadata":{"id":"f5yXpRQMXopU"},"source":["\n","\n","---\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Chatbot_code_in_PYTHON.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}
